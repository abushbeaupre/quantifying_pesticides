@article{antweilerEvaluationStatisticalTreatments2008,
  title = {Evaluation of {{Statistical Treatments}} of {{Left-Censored Environmental Data}} Using {{Coincident Uncensored Data Sets}}: {{I}}. {{Summary Statistics}}},
  shorttitle = {Evaluation of {{Statistical Treatments}} of {{Left-Censored Environmental Data}} Using {{Coincident Uncensored Data Sets}}},
  author = {Antweiler, Ronald C. and Taylor, Howard E.},
  year = {2008},
  month = may,
  journal = {Environmental Science \& Technology},
  volume = {42},
  number = {10},
  pages = {3732--3738},
  issn = {0013-936X, 1520-5851},
  doi = {10.1021/es071301c},
  urldate = {2024-11-11},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\ULG9ZAV2\Antweiler and Taylor - 2008 - Evaluation of Statistical Treatments of Left-Censo.pdf}
}

@article{busschaertHierarchicalBayesianAnalysis2011,
  title = {Hierarchical {{Bayesian}} Analysis of Censored Microbiological Contamination Data for Use in Risk Assessment and Mitigation},
  author = {Busschaert, P. and Geeraerd, A.H. and Uyttendaele, M. and Van Impe, J.F.},
  year = {2011},
  month = jun,
  journal = {Food Microbiology},
  volume = {28},
  number = {4},
  pages = {712--719},
  issn = {07400020},
  doi = {10.1016/j.fm.2010.06.006},
  urldate = {2024-11-11},
  abstract = {Microbiological contamination data often is censored because of the presence of non-detects or because measurement outcomes are known only to be smaller than, greater than, or between certain boundary values imposed by the laboratory procedures. Therefore, it is not straightforward to fit distributions that summarize contamination data for use in quantitative microbiological risk assessment, especially when variability and uncertainty are to be characterized separately. In this paper, distributions are fit using Bayesian analysis, and results are compared to results obtained with a methodology based on maximum likelihood estimation and the non-parametric bootstrap method. The Bayesian model is also extended hierarchically to estimate the effects of the individual elements of a covariate such as, for example, on a national level, the food processing company where the analyzed food samples were processed, or, on an international level, the geographical origin of contamination data. Including this extra information allows a risk assessor to differentiate between several scenario's and increase the specificity of the estimate of risk of illness, or compare different scenario's to each other. Furthermore, inference is made on the predictive importance of several different covariates while taking into account uncertainty, allowing to indicate which covariates are influential factors determining contamination.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\GH9WITWZ\Busschaert et al. - 2011 - Hierarchical Bayesian analysis of censored microbi.pdf}
}

@article{ComparisonMethodsAnalyzing2014,
  title = {Comparison of {{Methods}} for {{Analyzing Left-Censored Occupational Exposure Data}}},
  year = {2014},
  month = sep,
  journal = {The Annals of Occupational Hygiene},
  issn = {1475-3162},
  doi = {10.1093/annhyg/meu067},
  urldate = {2024-11-11},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\GWMW2ASS\2014 - Comparison of Methods for Analyzing Left-Censored .pdf}
}

@article{ComparisonSeveralMethods2007,
  title = {A {{Comparison}} of {{Several Methods}} for {{Analyzing Censored Data}}},
  year = {2007},
  month = oct,
  journal = {The Annals of Occupational Hygiene},
  issn = {1475-3162},
  doi = {10.1093/annhyg/mem045},
  urldate = {2024-11-11},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\FNBP7RXA\2007 - A Comparison of Several Methods for Analyzing Cens.pdf}
}

@article{finkelsteinExposureEstimationPresence2001,
  title = {Exposure {{Estimation}} in the {{Presence}} of {{Nondetectable Values}}: {{Another Look}}},
  shorttitle = {Exposure {{Estimation}} in the {{Presence}} of {{Nondetectable Values}}},
  author = {Finkelstein, Murray M. and Verma, Dave K.},
  year = {2001},
  month = mar,
  journal = {AIHAJ - American Industrial Hygiene Association},
  volume = {62},
  number = {2},
  pages = {195--198},
  issn = {1529-8663},
  doi = {10.1080/15298660108984622},
  urldate = {2024-11-11},
  abstract = {A common problem faced by industrial hygienists is the selection of a valid way of dealing with those samples reported to contain nondetectable values of the contaminant. In 1990, Hornung and Reed compared a maximum likelihood estimation (MLE) statistical method and two methods involving the limit of detection, L. The MLE method was shown to produce unbiased estimates of both the mean and standard deviation under a variety of conditions. That method, however, was complicated, requiring difficult mathematical calculations. Two simpler alternatives involved the substitution of L/2 or L/͙2 for each nondetectable value. The L/͙2 method was recommended when the data were not highly skewed. Although the MLE method produces the best estimates of the mean and standard deviation of an industrial hygiene data set containing values below the detection limit, it was not practical to recommend this method in 1990. However, with advances in desktop computing in the past decade the MLE method is now easily implemented in commonly available spreadsheet software. This article demonstrates how this method may be implemented using spreadsheet software.},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\WY49EBG9\Finkelstein and Verma - 2001 - Exposure Estimation in the Presence of Nondetectab.pdf}
}

@article{ganserAccurateSubstitutionMethod2010,
  title = {An {{Accurate Substitution Method}} for {{Analyzing Censored Data}}},
  author = {Ganser, Gary H. and Hewett, Paul},
  year = {2010},
  month = feb,
  journal = {Journal of Occupational and Environmental Hygiene},
  volume = {7},
  number = {4},
  pages = {233--244},
  issn = {1545-9624, 1545-9632},
  doi = {10.1080/15459621003609713},
  urldate = {2024-11-11},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\ACAZN5UF\Ganser and Hewett - 2010 - An Accurate Substitution Method for Analyzing Cens.pdf}
}

@article{gegenschatzBindingGapExperiments2022,
  title = {Binding the Gap between Experiments, Statistics, and Method Comparison: {{A}} Tutorial for Computing Limits of Detection and Quantification in Univariate Calibration for Complex Samples},
  shorttitle = {Binding the Gap between Experiments, Statistics, and Method Comparison},
  author = {Gegenschatz, Sof{\'i}a A. and Chiappini, Fabricio A. and Teglia, Carla M. and {Mu{\~n}oz de la Pe{\~n}a}, Arsenio and Goicoechea, H{\'e}ctor C.},
  year = {2022},
  month = may,
  journal = {Analytica Chimica Acta},
  volume = {1209},
  pages = {339342},
  issn = {0003-2670},
  doi = {10.1016/j.aca.2021.339342},
  urldate = {2024-10-25},
  abstract = {The present tutorial aims to review the most frequently reported criteria for the calculation of the limits of detection (LOD) and quantification (LOQ) in univariate calibration, summarizing their fundamentals, advantages, and limitations. The current criteria for estimating LOD and LOQ are based on diverse theoretical and/or empirical assumptions and require different amounts of experimental data, making the calculation rather complex in some cases. Moreover, alternative forms for calculating LOD/LOQ frequently lead to dissimilar results. This scenario might worsen in the case of complex analytical systems. Throughout this tutorial, different forms of calculating LOD/LOQ are illustrated using previously reported experimental datasets in the environmental chemistry field as examples. The influence of the sample matrix during the estimation of LOD/LOQ parameters is investigated through one calibration approache. The discrepancies in the obtained results with different criteria for the calculation of LOD/LOQ are highlighted. Finally, general guidelines and recommendations regarding experimental and data processing issues are proposed, aiming to promote fair criteria for the comparison of different analytical methodologies in terms of prediction ability and detection capability.},
  keywords = {Blank,complex analytical systems,Limit of detection (LOD),Limit of quantification (LOQ),Method comparison,univariate calibration},
  file = {C:\Users\allen\Zotero\storage\8TXP3KAH\S0003267021011685.html}
}

@article{gilliomEstimationDistributionalParameters1986,
  title = {Estimation of {{Distributional Parameters}} for {{Censored Trace Level Water Quality Data}}: 1. {{Estimation Techniques}}},
  shorttitle = {Estimation of {{Distributional Parameters}} for {{Censored Trace Level Water Quality Data}}},
  author = {Gilliom, Robert J. and Helsel, Dennis R.},
  year = {1986},
  month = feb,
  journal = {Water Resources Research},
  volume = {22},
  number = {2},
  pages = {135--146},
  issn = {0043-1397, 1944-7973},
  doi = {10.1029/WR022i002p00135},
  urldate = {2024-11-11},
  abstract = {A recurring difficulty encountered in investigations of many metals and organic contaminants in ambient waters is that a substantial portion of water sample concentrations are below limits of detection established by analytical laboratories. Several methods were evaluated for estimating distributional parameters for such censored data sets using only uncensored observations. Their reliabilities were evaluated by a Monte Carlo experiment in which small samples were generated from a wide range of parent distributions and censored at varying levels. Eight methods were used to estimate the mean, standard deviation, median, and interquartile range. Criteria were developed, based on the distribution of uncensored observations, for determining the best performing parameter estimation method for any particular data set. The most robust method for minimizing error in censored-sample estimates of the four distributional parameters over all simulation conditions was the log-probability regression method. With this method, censored observations are assumed to follow the zero-to-censoring level portion of a lognormal distribution obtained by a least squares regression between logarithms of uncensored concentration observations and their               z               scores. When method performance was separately evaluated for each distributional parameter over all simulation conditions, the log-probability regression method still had the smallest errors for the mean and standard deviation, but the lognormal maximum likelihood method had the smallest errors for the median and interquartile range. When data sets were classified prior to parameter estimation into groups reflecting their probable parent distributions, the ranking of estimation methods was similar, but the accuracy of error estimates was markedly improved over those without classification.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\BLM9IW6T\Gilliom and Helsel - 1986 - Estimation of Distributional Parameters for Censor.pdf}
}

@article{hornungEstimationAverageConcentration1990,
  title = {Estimation of {{Average Concentration}} in the {{Presence}} of {{Nondetectable Values}}},
  author = {Hornung, Richard W. and Reed, Laurence D.},
  year = {1990},
  month = jan,
  journal = {Applied Occupational and Environmental Hygiene},
  volume = {5},
  number = {1},
  pages = {46--51},
  issn = {1047-322X, 1521-0898},
  doi = {10.1080/1047322X.1990.10389587},
  urldate = {2024-11-11},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\DH4N3IAY\Hornung and Reed - 1990 - Estimation of Average Concentration in the Presenc.pdf}
}

@article{huynhComparisonVSubstitutionMethod2015,
  title = {A {{Comparison}} of the {$\beta$}-{{Substitution Method}} and a {{Bayesian Method}} for {{Analyzing Left-Censored Data}}},
  author = {Huynh, Tran and Quick, Harrison and Ramachandran, Gurumurthy and Banerjee, Sudipto and Stenzel, Mark and Sandler, Dale P. and Engel, Lawrence S. and Kwok, Richard K. and Blair, Aaron and Stewart, Patricia A.},
  year = {2015},
  month = jul,
  journal = {Annals of Occupational Hygiene},
  pages = {mev049},
  issn = {0003-4878, 1475-3162},
  doi = {10.1093/annhyg/mev049},
  urldate = {2024-11-11},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\BJH8BRR9\Huynh et al. - 2015 - A Comparison of the β-Substitution Method and a Ba.pdf}
}

@article{klymusReportingLimitsDetection2020,
  title = {Reporting the Limits of Detection and Quantification for Environmental {{DNA}} Assays},
  author = {Klymus, Katy E. and Merkes, Christopher M. and Allison, Michael J. and Goldberg, Caren S. and Helbing, Caren C. and Hunter, Margaret E. and Jackson, Craig A. and Lance, Richard F. and Mangan, Anna M. and Monroe, Emy M. and Piaggio, Antoinette J. and Stokdyk, Joel P. and Wilson, Chris C. and Richter, Catherine A.},
  year = {2020},
  journal = {Environmental DNA},
  volume = {2},
  number = {3},
  pages = {271--282},
  issn = {2637-4943},
  doi = {10.1002/edn3.29},
  urldate = {2024-10-24},
  abstract = {Background Environmental DNA (eDNA) analysis is increasingly being used to detect the presence and relative abundance of rare species, especially invasive or imperiled aquatic species. The rapid progress in the eDNA field has resulted in numerous studies impacting conservation and management actions. However, standardization of eDNA methods and reporting across the field is yet to be fully established, with one area being the calculation and interpretation of assay limit of detection (LOD) and limit of quantification (LOQ). Aims Here, we propose establishing consistent methods for determining and reporting of LOD and LOQ for single-species quantitative PCR (qPCR) eDNA studies. Materials \& Methods/ Results We utilize datasets from multiple cooperating laboratories to demonstrate both a discrete threshold approach and a curve-fitting modeling approach for determining LODs and LOQs for eDNA qPCR assays. We also provide details of an R script developed and applied for the modeling method. Discussion/Conclusions Ultimately, standardization of how LOD and LOQ are determined, interpreted, and reported for eDNA assays will allow for more informed interpretation of assay results, more meaningful interlaboratory comparisons of experiments, and enhanced capacity for assessing the relative technical quality and performance of different eDNA qPCR assays.},
  langid = {english},
  keywords = {assay optimization,eDNA,qPCR,standardization},
  file = {C:\Users\allen\Zotero\storage\7XNHMEEW\edn3.html}
}

@article{sheANALYZINGCENSOREDWATER1997,
  title = {{{ANALYZING CENSORED WATER QUALITY DATA USING A NON}}-{{PARAMETRIC APPROACH}}{\textsuperscript{1}}},
  author = {She, Nian},
  year = {1997},
  month = jun,
  journal = {JAWRA Journal of the American Water Resources Association},
  volume = {33},
  number = {3},
  pages = {615--624},
  issn = {1093-474X, 1752-1688},
  doi = {10.1111/j.1752-1688.1997.tb03536.x},
  urldate = {2024-11-11},
  abstract = {In the analysis of water quality data, samples with concentrations reported below the limit of detection (LOD) are referred to as Type I censored on the left. A variety of procedures have been proposed for estimating descriptive statistics from leftcensored data. Usually, the estimation is carried out by either replacing the LOD with a constant between 0 and the LOD, or assuming the data follow a normal or lognormal distribution. In this paper, a simple transformation is proposed to convert multiple left-censored water quality data to right-censored data. The transformed cumulative distribution is similarto a survival function, and enables use of survival analysis techniques for left-censored data. In particular, the product limit method (Kaplan-Meier estimator) is applied to estimate descriptive statistics from the transformed data. The performance of the Kaplan-Meier estimator is compared with maximum likelihood, probability plotting, and substitution methods by Monte Carlo simulations. The Kaplan-Meier estimator performs as well as or better than these more familiar methods. Finally, the Kaplan-Meier estimator is used to analyze some priority pollutant data collected in sediment from the central basin of Puget Sound.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\5Z66IAID\She - 1997 - ANALYZING CENSORED WATER QUALITY DATA USING A NON‐.pdf}
}

@article{shoariImprovedAnalysisConcentration2018,
  title = {Toward Improved Analysis of Concentration Data: {{Embracing}} Nondetects},
  shorttitle = {Toward Improved Analysis of Concentration Data},
  author = {Shoari, Niloofar and Dub{\'e}, Jean-S{\'e}bastien},
  year = {2018},
  month = mar,
  journal = {Environmental Toxicology and Chemistry},
  volume = {37},
  number = {3},
  pages = {643--656},
  issn = {0730-7268, 1552-8618},
  doi = {10.1002/etc.4046},
  urldate = {2024-11-11},
  abstract = {Abstract                                                            Various statistical tests on concentration data serve to support decision-making regarding characterization and monitoring of contaminated media, assessing exposure to a chemical, and quantifying the associated risks. However, the routine statistical protocols cannot be directly applied because of challenges arising from nondetects or left-censored observations, which are concentration measurements below the detection limit of measuring instruments. Despite the existence of techniques based on survival analysis that can adjust for nondetects, these are seldom taken into account properly. A comprehensive review of the literature showed that managing policies regarding analysis of censored data do not always agree and that guidance from regulatory agencies may be outdated. Therefore, researchers and practitioners commonly resort to the most convenient way of tackling the censored data problem by substituting nondetects with arbitrary constants prior to data analysis, although this is generally regarded as a bias-prone approach. Hoping to improve the interpretation of concentration data, the present article aims to familiarize researchers in different disciplines with the significance of left-censored observations and provides theoretical and computational recommendations (under both frequentist and Bayesian frameworks) for adequate analysis of censored data. In particular, the present article synthesizes key findings from previous research with respect to 3 noteworthy aspects of inferential statistics: estimation of descriptive statistics, hypothesis testing, and regression analysis.                 Environ Toxicol Chem                 2018;37:643--656. {\copyright} 2017 SETAC},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\YFDVBZJ4\Shoari and Dubé - 2018 - Toward improved analysis of concentration data Em.pdf}
}

@article{shoariInvestigationImpactLeftcensored2016,
  title = {An Investigation of the Impact of Left-censored Soil Contamination Data on the Uncertainty of Descriptive Statistical Parameters},
  author = {Shoari, Niloofar and Dub{\'e}, Jean-S{\'e}bastien},
  year = {2016},
  month = oct,
  journal = {Environmental Toxicology and Chemistry},
  volume = {35},
  number = {10},
  pages = {2623--2631},
  issn = {0730-7268, 1552-8618},
  doi = {10.1002/etc.3420},
  urldate = {2024-11-11},
  abstract = {Abstract                                                            Left-censored concentration data are frequently encountered because measuring instruments cannot detect concentrations below the instrument detection limit. For statistical analysis of left-censored data, the environmental literature mainly refers to the following methods: maximum likelihood estimator, regression on order statistics using log-normal and gamma assumption (rROS and GROS, respectively), and Kaplan--Meier. A number of simulation experiments examined the performance of these methods in terms of bias and/or mean square error. However, no matter which method is adopted, some uncertainty is introduced into outcomes because all that is known about a left-censored observation is that the concentration falls between 0 and the detection limit. The data used in the present study come from analysis of soil samples collected for a site characterization in Montreal, Canada. Employing nonparametric bootstrap, the authors quantify the uncertainty and bias in the mean and standard deviation estimates obtained by the maximum likelihood estimation (under log-normal, Weibull, and gamma distributions), rROS, GROS, and Kaplan--Meier methods. First, the authors demonstrate that the highest uncertainty is associated with the maximum likelihood estimator under log-normality and Weibull assumptions, whereas a gamma assumption leads to estimates with less uncertainty. Second, the authors show that although an increase in sample size improves the uncertainty, it reduces the bias only in the rROS, GROS, and Kaplan--Meier methods. Finally, comparing percentage uncertainty in the mean of contaminant data, the authors illustrate that adopting an inappropriate estimator results in large uncertainties.                 Environ Toxicol Chem                 2016;35:2623--2631. {\copyright} 2016 SETAC},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\WQRX7ZUS\Shoari and Dubé - 2016 - An investigation of the impact of left‐censored so.pdf}
}

@article{sinhaEvaluationStatisticalMethods2006,
  title = {Evaluation of Statistical Methods for Left-censored Environmental Data with Nonuniform Detection Limits},
  author = {Sinha, Parikhit and Lambert, Michael B. and Trumbull, V. Lyle},
  year = {2006},
  month = sep,
  journal = {Environmental Toxicology and Chemistry},
  volume = {25},
  number = {9},
  pages = {2533--2540},
  issn = {0730-7268, 1552-8618},
  doi = {10.1897/05-548R.1},
  urldate = {2024-11-11},
  abstract = {Monte Carlo simulations were used to evaluate statistical methods for estimating 95\% upper confidence limits of mean constituent concentrations for left-censored data with nonuniform detection limits. Two primary scenarios were evaluated: data sets with 15 to 50\% nondetected samples and data sets with 51 to 80\% nondetected samples. Sample size and the percentage of nondetected samples were allowed to vary randomly to generate a variety of left-censored data sets. All statistical methods were evaluated for efficacy by comparing the 95\% upper confidence limits for the left-censored data with the 95\% upper confidence limits for the noncensored data and by determining percent coverage of the true mean (␮). For data sets with 15 to 50\% nondetected samples, the trimmed mean, Winsorization, Aitchison's, and log-probit regression methods were evaluated. The log-probit regression was the only method that yielded sufficient coverage (99--100\%) of ␮, as well as a high correlation coefficient (r2 ϭ 0.99) and small average percent residuals (Ϫ0.1\%) between upper confidence limits for censored versus noncensored data sets. For data sets with 51 to 80\% nondetected samples, a bounding method was effective (r2 ϭ 0.96--0.99, average residual ϭ Ϫ5\% to Ϫ7\%, 95--98\% coverage of ␮), except when applied to distributions with low coefficients of variation (standard deviation/␮ Ͻ 0.5). Thus, the following recommendations are supported by this research: data sets with 15 to 50\% nondetected samples---log-probit regression method and use of Chebyshev theorem to estimate 95\% upper confidence limits; data sets with 51 to 80\% nondetected samples---bounding method and use of Chebyshev theorem to estimate 95\% upper confidence limits.},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\RUSNJTSS\Sinha et al. - 2006 - Evaluation of statistical methods for left‐censore.pdf}
}

@article{succopImputationDataValues2004,
  title = {Imputation of {{Data Values That}} Are {{Less Than}} a {{Detection Limit}}},
  author = {Succop, Paul A. and Clark, Scott and Chen, Mei and Galke, Warren},
  year = {2004},
  month = jul,
  journal = {Journal of Occupational and Environmental Hygiene},
  volume = {1},
  number = {7},
  pages = {436--441},
  issn = {1545-9624, 1545-9632},
  doi = {10.1080/15459620490462797},
  urldate = {2024-11-11},
  abstract = {Results of the analyses of occupational and environmental samples are frequently reported as ``less than a specified value,'' a practice followed by many analytical laboratories. A left-censored distribution occurs when analytical laboratories do not report results that fall below their limits of detection or quantification. Approximately 37\% of the household interior dust lead loadings collected in a large-scale, multisite, longitudinal study of lead-based paint hazard controls were reported to be below the ``method detection limit.'' These unreported values are unusable in any statistical analysis of the data and must be replaced by a valid dust lead loading estimate, a process called data imputation. This investigation tested how well data imputed using a newly formulated procedure for estimating the data below the method detection limit were correlated with dust lead loadings reported by the participating laboratories after special request. These results were also compared with those obtained by imputing the minimum detectable level by the square root of 2. Imputation of the low lead loadings was accomplished by substituting the value associated with the median percentile below each laboratory's method detection limit. A correlation of r = 0.50 was calculated between the predicted and reported dust lead loadings, with only slight bias (2.9\%) in the predicted values. An alternative imputation procedure that used the predicted value from structural equation models fit to the noncensored dust lead loadings performed about as well, although the predictions had to be ``centered'' to correspond to the censored data. An estimator that combined both of these imputation procedures only slightly improved the correlation between the predicted and laboratory values (r = 0.51). These results support the use of the new procedure rather than the commonly used imputed values of the method detection limit divided by 2 or by the square root of 2. Imputing values based on either of these common approaches may result in much more biased predictions for the censored data; in the case of these data, the dust lead loadings were overestimated by 348\%. The results also suggest that analytical laboratories should provide a numerical result for all analyzed samples, with a ``flag'' of those values below their detection limit, since these results may be more accurate than any imputed value, particularly those provided by the commonly used method of dividing the minimum detection limit by the square root of 2.},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\UIGYEVDA\Succop et al. - 2004 - Imputation of Data Values That are Less Than a Det.pdf}
}

@article{vlachonikolisEvaluationCensoredContamination1995,
  title = {Evaluation of Censored Contamination Data{\dag}},
  author = {Vlachonikolis, I. G. and Marriott, F. H. C.},
  year = {1995},
  month = sep,
  journal = {Food Additives and Contaminants},
  volume = {12},
  number = {5},
  pages = {637--644},
  issn = {0265-203X},
  doi = {10.1080/02652039509374352},
  urldate = {2024-11-11},
  langid = {english},
  file = {C:\Users\allen\Zotero\storage\MFQEW6WB\Vlachonikolis and Marriott - 1995 - Evaluation of censored contamination data†.pdf}
}
