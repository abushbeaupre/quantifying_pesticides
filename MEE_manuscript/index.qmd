---
title: Towards a principled statistical workflow for the study of wildlife and environmental contaminants in the presence of method detection and quantification limits
authors:
  - name: Allen Bush-Beaupré
    affiliation: Université de Sherbrooke
    roles: conception, writing, data analysis
    corresponding: true
    orcid: 0000-0001-6989-9278
    email: allen.bush-beaupre@usherbrooke.ca
  - name: Marc Bélisle
    affiliation: Université de Sherbrooke
    roles: conception, funding acquisition, writing
    orcid: 0000-0003-0657-6724
bibliography: references.bib
csl: apa.csl  
format:
  pdf:
    mainfont: DejaVuSans  
    monofont: DejaVuSansMono
    number-sections: true
    documentclass: article
    geometry: margin=1in
    cite-method: natbib
---

```{r echo=FALSE}
library(tidyverse)     # Data manipulation and visualization
library(geomtextpath)  # Curved text annotations
library(knitr)         # Table formatting
library(kableExtra)    # Table formatting
library(brms)          # Bayesian GLMMs
library(cmdstanr)      # cmdstanr backend for brms
library(tidybayes)     # Extract and visualize priors/posteriors 
set.seed(333)          # Set seed for reproducibility

# Function to calculate a parameter of lognormal distribution
calculate_a_lnorm <- function(mean, sd){

  log(mean^2/sqrt(sd^2 + mean^2))

}

# Function to calculate b parameter of lognormal distribution
calculate_b_lnorm <- function(mean, sd){

  sqrt(log(sd^2 / mean^2 + 1))

}

# Function to convert output of epred_ functions to obtain the expected mean when present
calculate_exp_mean <- function(epred, hu){

    epred / (1 - hu)

}

# Function to convert output of epred_function to obtain the expected sd when present
calculate_exp_sd<- function(mean, a){

    mean * sqrt((mean^2 - exp(2*a)) / exp(2*a))

}

# Define and set the global theme
custom_theme <- theme(
  axis.line = element_line(linewidth = 2, lineend = "round"),
  panel.grid = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  axis.text = element_text(size = 14, face = "bold"),
  axis.title = element_text(size = 16, face = "bold"),
  legend.text = element_text(size = 12, face = "bold"),
  legend.title = element_text(size = 14, face = "bold"),
  plot.title = element_text(size = 17, face = "bold"),
  plot.caption = element_text(face = "bold"),
  strip.background = element_rect(fill = "white"),
  strip.text = element_text(size = 12, face = "bold")
)

theme_set(custom_theme)
```

## Introduction

Human industrial and agricultural intensification has resulted in the release of an astronomical diversity and quantity of contaminants in the environment. The impact of these contaminants, such as mercury and pesticides used in agriculture has been the subject of many discussions in the scientific and public sphere. In her seminal book, Rachel Carson BLABLA. Much effort has been invested in quantifying the impact of such contaminants on various life traits in controlled conditions (eg. BLABLA). Studies such as these are a critical first step in determining contaminant concentration thresholds at which we may observe the detrimental impact of human industrial and agricultural activities. However, the results of these laboratory and semi-field studies can be difficult to fully extrapolate to the real-life environment conditions to which organisms are exposed. Organisms are exposed to a diversity of contaminants depending on their environmental conditions. Additionally, we would expect an increased level of contamination in organisms at higher trophic levels due to bioaccumulation.

As emphasized in a review by Tison et al. (2024), the mechanisms and consequences by which pesticides accumulate across trophic levels are understudied. The initial step in studying such phenomena is a proper quantification of the chemicals in collected samples. Contamination concentrations are obtained through a variety of laboratory methods. These methods, however, have varying degrees of precision especially when concentrations are low. Thus arises the limit of detection (LOD), the concentration threshold under which the analytical method can not accurately determine if the contaminant is present in the sample or not. Additionally, many analytical methods commonly used have a limit of quantification (LOQ) such that concentrations between the LOD and LOQ are known to be present, but the actual concentration is only known to be within the aforementioned interval. As will be exemplified here, much effort has been made to deal with these complicated data to generate meaningful statistics for over 50 years (ie Leese, 1973). Methods have been proposed, validated and their performance compared by multiple authors (ie Ganser & Hewett, 2010; Huynh et al. 2014, 2015; Busschaert et al. 2011; Gilliom, 1986; She, 1997; Hornung & Reed, 1990; Antweiler & Taylor, 2008; Hewett & Ganser, 2007; Shoari 2016; Sinha, 2006; Succop et al. 2004; Vlachonikolis & Marriott, 1995; ). However, practitioners in the field of ecology often lack the statistical training to choose and implement an appropriate method for their specific use cases. Authors often refer to heuristics proposed by others through simulation studies without considering the specific conditions involved in those simulations. We propose here a general data analysis workflow that may be adapted by practitioners for their data at hand using the most recent advances in statistical software and methods. Such workflow will encourage researchers to tailor their statistical models to the specific characteristics of their collected data and make for easier contrasts among results obtained across various studies.

The accuracy and relevance of the results obtained from modeling efforts rely on statistical models based on the data-generating process. Such endeavor implies profound thinking about the characteristics of the data collected. An example of this in regression models is the use of likelihood distributions mathematically sound for the data at hand. The lognormal distribution is often recommended for concentration data as BLABLA REFS. In the case of contaminants found in wildlife samples, however, for a concentration level to be known, the contaminant must be present in the sample in the first place. Thus arises a need for quantifying the probability of contamination along with the concentration level if present. Generalized Linear Model (GLM) likelihoods that may account for this process are part of the hurdle or zero-inflated class of models. Such models are used, for example, in occupancy and distribution models (refs). EXAMPLE REF. An additional aspect of the data-generating process is the limitations imposed by the methods for detection and quantification of contaminants (the previously mentioned LOD and LOQ). A proper modeling of the data-generating process mentioned here may at first glance appear complex and tedious. The next sections will focus on elaborating the formulation of a statistical model incorporating the entirety of these specific data characteristics with reference to approaches taken by researchers in recent publications (2020-2024) to deal with contaminant presence (often termed exposure) and concentrations in the presence of LODs and LOQs.

### Data-generating process: limits of detection and quantification

Concentration values under LOD and between LOD and LOQ are considered as censored observations. Censoring in the context of data analysis refers to inferring BLABLA. These methods are often used in survival analysis where, for example, we attempt to estimate the survival time of an individual in a population, but we don’t know exactly when death happened. Such cases arise when we observe individuals over time but do not get the opportunity to confirm its time of death due to a variety of reasons such as it leaving the study site, or the study period did not last long enough to observe the individual’s death. Such data are said to be right-censored as we do not observe certain values on the right-side of the distribution of survival times. Although we did not observe the time of death, we know the individual survived at least until the last time we recorded it. ACTUAL EXAMPLE.

There are two other cases of data censoring. Left-censoring occurs when we do not observe values past the left side of a given threshold of a distribution. This may occur, to return to the previous survival time example, when we do not know the exact age of an individual within a population and the start of the study. We do, however, have methods to approximate its maximal age and thus know that the age of the individual must be below a specific threshold. Lastly, interval-censoring arises when, for example, an event occurs (such as death) between two sampling occasions. We thus know that death occurred between two values of time but are uncertain as to exactly when.

Multiple statistical methods have been developed and used for such data, with varying levels of impact on the estimation of parameters (such as the mean). Shaori & Dubé (2018) reviewed various frequentist and Bayesian methods for this statistical challenge. Readers are referred to the aforementioned review for a detailed overview. We focus here mainly on common practices still observed in recent years by wildlife and environmental practitioners and build up the reader’s intuition for understanding such data with the objective to fit regression models evaluating the impact of certain predictors on contaminant presence and concentration.

### Ignoring values under LOD and/or LOQ

One way to work with data under the LOD and/or LOQ is to make the assumption that these small values are not biologically relevant for the context at hand. For example, Albert et al. (2021) measured levels of Mercury in Arctic seabirds and quantified concentration levels several orders of magnitude higher than the LOD. In this case, the authors’ assumptions may be justified as the number of samples with concentration values below the LOD would be unlikely to severely bias the estimation of concentration levels. There are many contexts, however, where such assumptions may be detrimental to the precision of the reported estimated concentrations. For example, when concentration values are expected to be low such as BLABLA (refs). Additionally, authors have previously highlighted the bias induced by such methods (Hashimoto & Trussel, 1983). While certain studies can justify the choice of data modifications using toxicological studies in laboratory settings with known contamination levels and thus use calculations based on samples exceeding a certain threshold concentration (eg. Odemer et al. 2023), this does not take into account the half life of these contaminants which can be low in certain cases (such as neonicotinoids; refs). Additionally, ignoring lower concentration levels does not aid in eventually quantifying the effects of these contaminants on the survival and health parameters of the studied organisms in the wild. Nonetheless, ignoring detection and/or quantification limits of contamination levels in wildlife studies remains a common occurrence in recent publications (Spadetto et al. 2024a, 2024b; Esther et al. 2022; Rial-Berriel et al. 2021; Martin-Cruz et al. 2024; Fuentes et al. 2023, 2024a, 2024b, 2024c; Bariod et al. 2024; Tison et al. 2023; Lennon et al. 2020b; Lesch et al. 2024; Badry et al. 2021; Elliot et al. 2024; Drummond et al. 2024).

### Data substitution or imputation

Some practitioners recognize the need to incorporate the information contained by the samples with concentrations below the LOD or LOQ and use a variety of methods to substitute values for these points. In recent years, researchers have substituted observations below either limit to the median between zero and the limit (Bishops et al. 2022; Elliot et al. 2022; Perkins et al. 2021), ignored data under the LOD and set values between LOD and LOQ to LOD (Rondeau et al. 2022) and fixed values below LOD to LOD (Lennon et al. 2020, Ito et al. 2020). Other practitioners have used various data imputation methods such as generating random values between zero and the LOD and between the LOD and LOQ (Martin-Cruz et al. 2024b). Bias produced by similar practices have been highlighted in previous publications (Hashimoto & Trussel, 1983). Another imputation method used is the β-substitution method (Ganser and Hewett, 2010), used, for example, by Eng et al. (2020).

### MLE

Shoari 2016 quantify uncertainty with bootstrap

There exists, however an alternative method to work with censored data in the Bayesian paradigm. Bayesian models treat uncertainty in a different manner than Frequentist ones and offer probabilistic interpretations of parameter values. In a Bayesian paradigm, censored observations are simply integrated over from the entire data distribution. This allows the uncertainty of the unobserved values to be propagated across the entire model and reflected in the width of confidence intervals generated. When comparing the β-substitution method to Bayesian left-censoring AUTHORS found the two methods to perform similarly at low levels of censoring. However, as the number of censored observations increased, Bayesian left-censoring had a higher predictive performance. Thus, Bayesian left-censoring allows to model censored data in a greater variety of cases. Researchers can thus use this method rather than not model concentrations at all when the level of censored observations is high such as in the case with METABOLITE in Eng et al. (2020). The method has been used in recent publications such as Kerric et al. (2022), MORE.

### Data-generating process: Contaminant presence

A measure that is often of interest is the probability of exposure that an environment or organism is subjected to. For example, researchers often discuss and analyse how the number of pesticides that an individual is exposed to affects certain life traits. Anderson et al. (2023) dichotomized samples above and below the LOQ as a measure of exposure probability. Due to a lower LOQ for Imidacloprid than the other pesticides tested, their method of interpretation leads to claiming that individuals had a higher exposure to Imidacloprid. When analysing a 30-year dataset of rodenticide contamination in raptors, Elliot et al. (2022) dealt with detection limits that ranged across several orders of magnitude by considering only the highest limit and thus quantifying “exposure” probability in relation to this value. However, discussing exposure in the aforementioned manner is erroneous; these measures relate to inherent imprecisions in the analytical method rather than the actual data-generating process. Nevertheless, such terminology is pervasive in recent literature (Rondeau et al. 2022; Fuentes et al. 2024a, 2024c; Rial-Berriel et al. 2021; Badry et al. 2021; Bischoff et al. 2023; Martinez et al. 2021; Rodriguez-Aguilar et al. 2022; Roy & Chen, 2023; Tison et al. 2023).

### Proposed general statistical framework

Thus far, we have highlighted four major characteristics of contamination concentration data as illustrated through a simulated data distribution in @fig-datagen. First, there are data that were properly quantified and are thus observed (ie non-censored; purple part of distribution). These observations describe the right tail of the distribution unambiguously. Second, we have observations that lie between the LOD and LOQ (ie interval-censored; gold part of distribution). Such data are ambiguous concerning their exact values but contain valuable information; we know what portion of the distribution within which they reside. Lastly, we have values below the LOD (left-censored; red part of distribution) which can be further characterised into zero-values and values between zero and the LOD. From the proportion of values located at zero, we may derive an exposure probability and, from all values above zero, we may derive an average concentration when the contaminant is present. Furthermore, one may be inclined to calculate a “overall” average thus incorporating the concentration when present and the contamination probability which can be obtained by multiplying the two metrics. Although it may appear counter-intuitive at first, we show here that it is possible to obtain these metrics from data with such characteristics in a variety of conditions. We emphasise, however, that practitioners evaluate the adaptability of the proposed method to their specific use-case following the workflow provided.

```{r fig-datagen, echo=FALSE, fig.height=6, fig.width=10, fig.cap="Data generating process of wildlife and environment sample contamination. The plot shows the distribution of simulated contaminant concentrations with different censoring regions highlighted."}
# Set simulation parameters
n <- 2000          # Number of samples
mean_resp <- 0.23  # Average on reponse scale
sd_resp <- 0.06    # Standard devation on reponse scale
p_zeros <- 0.1     # Probability of zero
LOD <- 0.15        # Limit of detection
LOQ <- 0.19        # Limit of quantification
# Calculate a and b parameters of lognormal distribution
a <- calculate_a_lnorm(mean_resp, sd_resp)
b <- calculate_b_lnorm(mean_resp, sd_resp)

# Generate simulated data
d <- tibble(
  conc = round(rlnorm(n, a, b),2),  # Generate lognormal concentrations
  zeros = rbinom(n, prob = (1 - p_zeros), size = 1), # Binary presence/absence
  Y = conc * zeros,  # Final concentrations (including zeros)
  cens = case_when(  # Classify censoring status
    Y <= LOD ~ "LOD",  # Left-censored
    Y > LOD & Y <= LOQ ~ "LOQ",  # Interval-censored
    .default = "obs"  # Fully observed
  ))


# Create a histogram plot with various annotations and customizations
ggplot() +
  # Add histogram with censoring categories
  geom_histogram(data = d,
                 aes(x = Y, 
                     y = ifelse(after_stat(count) > 0, after_stat(count), NA), # Remove line at zero where there are empty bins
                     fill = cens, 
                     color = cens),
                 binwidth = 0.01, 
                 lineend = "round", 
                 alpha = 0.7, 
                 linewidth = 1) +
  
  # Add vertical lines for LOD and LOQ
  geom_segment(aes(x = LOD + 0.006, 
                   y = 0, yend = 72), 
               color = "lightsalmon4", 
               lineend = "round", 
               linewidth = 2) +
  geom_segment(aes(x = LOQ + 0.006, 
                   y = 0, yend = 126), 
               color = "navajowhite4", 
               lineend = "round", 
               linewidth = 2) +
  
  # Add vertical line for sample mean
  geom_segment(aes(x = mean_resp,
                   y = 0, yend = 190),
               color = "steelblue4",
               lineend = "round",
               linetype = 3,
               linewidth = 1.5) +
  
  # Add curved text annotation for contaminant presence
  geom_textcurve(aes(x = 0.01, xend = 0.08,
                     y = 130, yend = 3),
                 label = "Contaminant presence",
                 color = "steelblue3",
                 fontface = "bold",
                 size = 4,
                 curvature = 0.3,
                 linewidth = 1.5,
                 angle = 120,
                 arrow = arrow(ends = "last", 
                               angle = 35, 
                               type = "closed",
                               length = (unit(0.12, "inches")))) +
  
  # Add text annotations for LOD and LOQ
  annotate(geom = "text",
           x = c(LOD + 0.006, LOQ + 0.006),
           y = -7,
           label = c("LOD", "LOQ"),
           size = 4.5,
           fontface = "bold",
           color = c("lightsalmon4", "navajowhite4")) +
  
  # Add text annotation for sample mean
  annotate(geom = "text",
           x = mean_resp,
           y = 197,
           label = "Actual average concentration",
           size = 4.5,
           fontface = "bold",
           color = "steelblue4") +
  
  # Customize x-axis breaks and labels
  scale_x_continuous(breaks = seq(0, 0.5, by = 0.1), 
                     labels = seq(0, 0.5, by = 0.1)) +
  
  # Add labels and title
  labs(x = "Simulated contaminant concentration",
       y = "Count",
       title = "Data generating process of wildlife and environment contamination",
       caption = "LOD = limit of detection, LOQ = limit of quantification") +
  
  # Customize fill and color scales for censoring categories
  scale_fill_manual(values = c("lightsalmon3", "navajowhite3", "thistle3"), 
                    name = "Censoring", 
                    labels = c("left", "interval", "none")) +
  scale_color_manual(values = c("lightsalmon3", "navajowhite3", "thistle3"), 
                     name = "Censoring", 
                     labels = c("left", "interval", "none")) +
  
  # Set x-axis limits
  coord_cartesian(xlim = c(0, 0.51)) +
  
  # Customize theme elements
  theme(legend.position = "inside",
        legend.position.inside = c(0.75, 0.7)
  ) -> fig_1  # Save plot to object fig_1

fig_1
```

Many of the aforementioned studies focus solely on the number of samples with detected levels of contaminants (refs). Treating the data in such a manner is problematic as previously emphasized. Others assume a lognormal distribution and may use any of the mentioned methods to deal (or not) with the censored data (refs). However, although the lognormal is theoretically sound for concentration data, it does not allow for zero-values. Thus, while such models may infer infinitely small concentrations, an important aspect of the data generating process is neglected; the probability that the contaminant is present in the sample in the first place. Model specifications to include the zero-generating process such as the hurdle lognormal are available and easily implemented using software such as brms. As will be shown using simulations and a case study of pesticide contamination in Tree Swallow boluses, a left- and interval censored hurdle lognormal bayesian hierarchical generalized linear model can be fit to such data to obtain contamination probability and concentration with acceptable levels of precision. As exemplified by the case study, the model can be complexified to include multiple pesticides along with correlations in their contamination probability and concentrations, measures of ‘cocktail’ contaminations.

Setting up a GLMM with censoring is not trivial and requires a deep understanding of the data-generating process. The next sections will focus on the formulation of the model and the interpretation of the results obtained. We will also provide a step-by-step guide to implement the model using the brms package in R.

## Data simulation

To illustrate the proposed statistical workflow, we will simulate a dataset of contaminant concentrations in wildlife samples. The data will be generated to mimic the characteristics of real-world data, including left-censored observations below the limit of detection (LOD) and interval-censored observations between the LOD and the limit of quantification (LOQ). The data will be generated from a lognormal distribution, which is commonly used to model concentration data. The simulation will also include a binary variable indicating the presence or absence of the contaminant in each sample. The data distribution generated can be observed in @fig-datagen.

Section on lognormal parameterization
{brms} models the A and B parameters

```{r, echo=TRUE, results='markup'}
# Set simulation parameters
n <- 2000          # Number of samples
mean_resp <- 0.23  # Average on reponse scale
sd_resp <- 0.06    # Standard devation on reponse scale
p_zeros <- 0.1     # Probability of zero
LOD <- 0.15        # Limit of detection
LOQ <- 0.19        # Limit of quantification
# Calculate a and b parameters of lognormal distribution
a <- calculate_a_lnorm(mean_resp, sd_resp)
b <- calculate_b_lnorm(mean_resp, sd_resp)
# Generate simulated data
censored_data <- tibble(
  # Generate lognormal concentrations
  conc = round(rlnorm(n, a, b),2),  
  # Presence probability of 0.9
  zeros = rbinom(n, prob = (1 - p_zeros), size = 1),  
  # Final concentrations (including zeros)
  Y = conc * zeros)                        
```

To analyse censored data, {brms} requires some pre-processing to recognize the censored observations. Values below LOD (left-censored) must be set to LOD while values between LOD and LOQ (interval-censored) must be set to the lower bound of the interval (LOD). Interval-censoring also requires additional column must be created to specify the upper bound of the interval censoring (LOQ) while data that are not censored are left as is. Finally, a column specifying the type of censoring (left, interval, none) must be created. The code below performs these operations on the simulated data.

```{r, echo = TRUE}
censored_data |>
  mutate(
    # format data
    y_cen = case_when(
    # Set values below LOD to LOD
      Y < LOD ~ LOD,  
    # Set values between LOD and LOQ to LOD
      Y >= LOD & Y < LOQ ~ LOD,  
    # Leave other values as is
      TRUE ~ Y),  
    # Create column for upper bound of interval censoring
    upper_int_cens = ifelse(Y >= LOD & Y < LOQ, LOQ, Y), 
    # Create column for censoring type
    censoring = case_when(
    # Left-censored
      Y < LOD ~ "left",  
    # Interval-censored
      Y >= LOD & Y < LOQ ~ "interval",  
    # Fully observed
      TRUE ~ "none")  
) -> censored_data
```

```{r, echo = FALSE, results = 'asis'}
censored_data |>
  select(Y, y_cen, upper_int_cens, censoring) |>
  slice_sample(n = 2, by = censoring) |>
  kableExtra::kable(format = "latex", booktabs = TRUE) |>
  kableExtra::kable_styling(latex_options = c("hold_position"))
```

## Model specification with {brms}
Next, we specifiy data censoring in the model equation with bf(censored observations | cens(censoring type, upper bound of interval censoring)). In bf_intercept below, we specify an intercept-only model for both the concentration and zero-probability with a hurdle-lognormal response distribution with identity link for the A parameter, log link for the B parameter and logit link for the zero-probability.

```{r, echo = TRUE, results = 'asis'}
bf_intercept <- bf(y_cen | cens(censoring , upper_int_cens) ~ 1, 
                                                         hu ~ 1, 
                           family = hurdle_lognormal(link = "identity", 
                                                     link_sigma = "log", 
                                                     link_hu = "logit")
)
```

As with any Bayesian model, sensible priors for every model parameter must be specified. As the a and b parameters of the lognormal distribution are calculated from the mean and SD of the observations, it can seem counter-intuitive to conceive of probable values for them. Thus, we suggest first thinking about probable values of the mean and SD of the obervations and then converting these prior distributions into priors for the a and b parameters. Supplementary material #1 displays code to explore the impact of different values for the mean and SD of the observations on the a and b parameters. While choosing prior distributions for these parameters this way is a good starting point, an addition method in evaluating the appropriateness of model priors is to simulate observations from the prior model before it sees the data. We can then evaluate if the observations appear reasonable for our use case.



# References {.unnumbered}

::: {#refs}
:::